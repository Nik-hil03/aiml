{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6949627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are taking AND operation and using backpropogation algorithm to classify the output as 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664b9060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Error: 0.5947693411886765\n",
      "Epoch 1000, Error: 0.36582384477199703\n",
      "Epoch 2000, Error: 0.17721425719878361\n",
      "Epoch 3000, Error: 0.09306961474037057\n",
      "Epoch 4000, Error: 0.0656786869368591\n",
      "Epoch 5000, Error: 0.05235073444058997\n",
      "Epoch 6000, Error: 0.0443546161073536\n",
      "Epoch 7000, Error: 0.03895553895260831\n",
      "Epoch 8000, Error: 0.035026769801154815\n",
      "Epoch 9000, Error: 0.032017243604984785\n",
      "\n",
      "Final predicted output:\n",
      "[[0.00633995]\n",
      " [0.03581228]\n",
      " [0.03393122]\n",
      " [0.95757809]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Imports the NumPy library, which provides support for array operations and numerical functions\n",
    "\n",
    "def sigmoid(x):  # Defines the sigmoid activation function\n",
    "    return 1 / (1 + np.exp(-x))  # Squashes input values to be between 0 and 1\n",
    "\n",
    "def sigmoid_derivative(x):  # Defines the derivative of the sigmoid function\n",
    "    return x * (1 - x)  # Computes the derivative used in backpropagation for gradient calculation\n",
    "\n",
    "# Input data for the AND gate (4 samples, each with 2 binary features)\n",
    "inputs = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# Expected output for an AND gate (1 output per input sample)\n",
    "labels = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "learning_rate = 0.1  # Sets the learning rate, which controls the update step size during training\n",
    "\n",
    "input_layer_neurons = inputs.shape[1]  # Sets the number of neurons in the input layer equal to the number of input features\n",
    "hidden_layer_neurons = 2               # Sets the number of neurons in the hidden layer (arbitrarily chosen)\n",
    "output_neurons = 1                      # Sets the number of neurons in the output layer to 1 (since it's a binary classification)\n",
    "\n",
    "# Initialize weights for connections between input and hidden layer\n",
    "weights_input_hidden = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
    "# Initialize bias for the hidden layer\n",
    "bias_hidden = np.random.uniform(size=(1, hidden_layer_neurons))\n",
    "\n",
    "# Initialize weights for connections between hidden and output layer\n",
    "weights_hidden_output = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n",
    "# Initialize bias for the output layer\n",
    "bias_output = np.random.uniform(size=(1, output_neurons))\n",
    "\n",
    "epochs = 10000  # Number of iterations (or passes) for training the neural network\n",
    "for epoch in range(epochs):  # Iterates over each epoch (training cycle)\n",
    "    # Forward pass: calculate hidden layer's weighted input and activation\n",
    "    hidden_layer_input = np.dot(inputs, weights_input_hidden) + bias_hidden  # Computes input for the hidden layer\n",
    "    hidden_layer_activation = sigmoid(hidden_layer_input)  # Applies sigmoid activation function to hidden layer input\n",
    "\n",
    "    # Calculate output layer's weighted input and activation\n",
    "    output_layer_input = np.dot(hidden_layer_activation, weights_hidden_output) + bias_output  # Computes input for the output layer\n",
    "    predicted_output = sigmoid(output_layer_input)  # Applies sigmoid activation function to output layer input\n",
    "\n",
    "    # Calculate the error as the difference between the predicted and actual output\n",
    "    error = labels - predicted_output  # Calculates error for output layer\n",
    "\n",
    "    # Backpropagation step: calculate gradient for output layer using the error and sigmoid derivative\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)  # Computes the gradient for output layer\n",
    "\n",
    "    # Calculate error for the hidden layer\n",
    "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)  # Backpropagates error to hidden layer\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_activation)  # Computes gradient for hidden layer\n",
    "\n",
    "    # Update weights and biases for hidden-to-output layer using the calculated gradient and learning rate\n",
    "    weights_hidden_output += hidden_layer_activation.T.dot(d_predicted_output) * learning_rate\n",
    "    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    #The .T in weights_hidden_output.T represents the transpose of the weights_hidden_output matrix.\n",
    "    #d_predicted_output is the gradient of the output layer, which has dimensions (n_samples, output_neurons).\n",
    "#weights_hidden_output is the matrix of weights between the hidden and output layers, with dimensions (hidden_layer_neurons, output_neurons).\n",
    "#To perform matrix multiplication (dot), the dimensions must align: the number of columns in the first matrix must match the number of rows in the second matrix.\n",
    "\n",
    "    # Update weights and biases for input-to-hidden layer using the calculated gradient and learning rate\n",
    "    weights_input_hidden += inputs.T.dot(d_hidden_layer) * learning_rate\n",
    "    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    # Every 1000 epochs, print the mean absolute error to monitor training progress\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Error: {np.mean(np.abs(error))}\")\n",
    "\n",
    "# Print the final predicted output after training completes\n",
    "print(\"\\nFinal predicted output:\")\n",
    "print(predicted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3710b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple implementation of a neural network using the backpropagation algorithm in Python. T\n",
    "his example uses NumPy for numerical computations and demonstrates how a basic neural network can learn to perform \n",
    "binary classification.\n",
    "\n",
    "Overview\n",
    "Architecture:\n",
    "\n",
    "Input Layer: 2 neurons\n",
    "Hidden Layer: 2 neurons\n",
    "Output Layer: 1 neuron\n",
    "Activation Function: Sigmoid function for both hidden and output layers.\n",
    "\n",
    "Dataset: Logical AND problem.\n",
    "\n",
    "Dataset\n",
    "Input 1\tInput 2\tOutput\n",
    "0\t0\t0\n",
    "0\t1\t0\n",
    "1\t0\t0\n",
    "1\t1\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024bcbd7",
   "metadata": {},
   "source": [
    "About bias:\n",
    "    In a neural network, bias is an additional parameter added to each neuron (or node) in a layer, allowing the model to have greater flexibility in fitting the data. \n",
    "    It’s a constant term added to the weighted sum of inputs before applying the activation function.\n",
    "Why Bias is Important\n",
    "Flexibility in Output Values: Without bias, the output of each neuron would always have to pass through the origin (0,0) when the inputs are zero, limiting the range of possible values and making it harder for the network to learn complex patterns.\n",
    "\n",
    "Shifting the Activation: The bias term shifts the activation function to help the network adjust the relationship between inputs and outputs. This is particularly important in classification tasks.\n",
    "\n",
    "Learning Patterns: With bias, the neuron can still activate (output non-zero) even when all inputs are zero, allowing it to learn patterns that wouldn’t be possible without it\n",
    "    \n",
    "bias_hidden = np.random.uniform(size=(1, hidden_layer_neurons))\n",
    "bias_output = np.random.uniform(size=(1, output_neurons))\n",
    "bias_hidden: A bias vector added to each neuron in the hidden layer.\n",
    "bias_output: A bias term added to the neuron(s) in the output layer.\n",
    "    \n",
    "During each forward pass, the bias is added to the weighted sum of inputs before the activation function is applied. For example, the line:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "hidden_layer_input = np.dot(inputs, weights_input_hidden) + bias_hidden\n",
    "\n",
    "adds bias_hidden to the weighted sum of inputs to each neuron in the hidden layer.\n",
    "\n",
    "Similarly, the output layer’s input calculation:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "output_layer_input = np.dot(hidden_layer_activation, weights_hidden_output) + bias_output\n",
    "adds bias_output to the weighted sum of the hidden layer’s activations before the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3828cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
